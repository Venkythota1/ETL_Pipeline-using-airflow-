# ETL_Pipeline-using-airflow
Data Pipeline Using Airflow to Pull Amazon Data Engineering Books and Store in PostgreSQL


**üß© Overview**

This project demonstrates how to build a data pipeline using Apache Airflow to scrape Amazon Data Engineering books and store the results in a PostgreSQL database.

All components are open-source and run locally using Docker containers.

**‚öôÔ∏è Architecture**

**Tools Used:**

Apache Airflow ‚Üí Workflow orchestration tool (scheduler)

PostgreSQL ‚Üí Relational database for storing data

pgAdmin ‚Üí GUI for managing PostgreSQL

Docker Compose ‚Üí To manage and run all containers together


Code to add in docker compose yaml file below services

"""

postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    ports:
      - "5432:5432"
pgadmin:
    container_name: pgadmin4_container2
    image: dpage/pgadmin4
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: root
    ports:
      - "5050:80"
"""




**
üß† Airflow Components**
Component	Description
Scheduler	:Schedules and triggers the execution of DAGs
Webserver :	Runs the Airflow UI for monitoring DAGs
Metadata DB	Stores DAG runs, task status, and configuration data
Executor :	Executes the tasks defined in the DAG

A DAG (Directed Acyclic Graph) represents a workflow ‚Äî a collection of tasks executed in a specific order.

üê≥ Setup: Airflow on Docker

We use Docker Compose to run Airflow, PostgreSQL, and pgAdmin containers together.
**
üîó Reference**

Official Airflow on Docker Documentation

**üßæ Steps**

Install Airflow on Docker

docker compose up airflow-init
docker compose up


Install PostgreSQL and pgAdmin on Docker
Both are included in the same docker-compose.yml file or can be added as separate services.

Verify Running Containers

docker ps
docker container ls


Inspect PostgreSQL Container
To connect from pgAdmin or Airflow:

docker inspect <container_id>


Copy the IP Address of the PostgreSQL container.

Connect pgAdmin to PostgreSQL
Use the container IP and port from the inspect command.

Create Database
In pgAdmin, create a new database named:

amazon_books

üìÑ DAG Description

The Airflow DAG performs the following tasks:

Extract

Scrapes book data (title, author, price, and rating) from Amazon.

Fetches top Data Engineering books.

Transform

Removes duplicates.

Cleans and formats the data into a Pandas DataFrame.

Load

Uses PostgresHook to connect to PostgreSQL.

Inserts the data into the books table.

**üß† Workflow Summary**
Step	Task	Description
1	scrape_books	Scrape book data from Amazon
2	clean_books	Clean and deduplicate data
3	load_to_postgres	Insert cleaned data into PostgreSQL using PostgresHook
üóÑÔ∏è Example Table Schema (books)
Column	Type	Description
title	TEXT	Book title
author	TEXT	Book author
price	FLOAT	Price of the book
rating	FLOAT	Average rating
üß∞ Airflow Connection Setup



**In the Airflow UI:**

Navigate to Admin ‚Üí Connections ‚Üí +

Create a new connection:

Conn Id: postgres_conn

Conn Type: Postgres

Host: <PostgreSQL container IP>

Schema: amazon_books

Login: postgres

Password: <your_password>

Port: 5432
